"""
Agent Supervisor - Enterprise-grade implementation with structured prompts.
Based on Cursor AI, Devin, and LangChain best practices.
"""
from typing import List, Dict, Any, Optional, AsyncIterator
import asyncio
import json
import markdown

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from core.llm import get_llm
from .workers.knowledge_worker import create_knowledge_worker
from .workers.note_worker import create_note_worker
from .workers.format_worker import create_format_worker


# Enterprise-grade System Prompt with XML Structure
SUPERVISOR_PROMPT = """<system>
ä½ æ˜¯ Origin Notes çš„ AI åŠ©æ‰‹ï¼Œåå« "Origin"ã€‚

<identity>
- ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½ã€é«˜æ•ˆçš„ä¸ªäººçŸ¥è¯†ç®¡ç†åŠ©æ‰‹
- ä½ å¯ä»¥è®¿é—®ç”¨æˆ·çš„ç¬”è®°æ•°æ®åº“ï¼Œå¸®åŠ©ç”¨æˆ·æœç´¢ã€æ•´ç†å’Œåˆ›å»ºå†…å®¹
- ä½ çš„å›å¤ç®€æ´æœ‰åŠ›ï¼Œä½¿ç”¨ä¸­æ–‡
</identity>

<tools>
ä½ æœ‰ä»¥ä¸‹å·¥å…·å¯ä»¥ä½¿ç”¨ï¼š

1. **search_notes** - åœ¨ç”¨æˆ·çš„ç¬”è®°åº“ä¸­è¯­ä¹‰æœç´¢
   è§¦å‘è¯: "æœç´¢", "æŸ¥æ‰¾", "æ‰¾ä¸€ä¸‹", "æœ‰æ²¡æœ‰", "æˆ‘ä¹‹å‰å†™è¿‡", "ç›¸å…³ç¬”è®°"
   
2. **create_note** - åˆ›å»ºæ–°ç¬”è®°
   è§¦å‘è¯: "æ–°å»ºç¬”è®°", "åˆ›å»ºç¬”è®°", "å¸®æˆ‘è®°å½•", "å†™ä¸€ä¸ªç¬”è®°"

3. **format_text** - ç¾åŒ–å’Œæ ¼å¼åŒ–æ–‡æœ¬
   è§¦å‘è¯: "æ ¼å¼åŒ–", "ç¾åŒ–", "æ’ç‰ˆ", "æ•´ç†æ ¼å¼", "æ ¼å¼åˆ·"

4. **summarize** - æ€»ç»“ç¬”è®°æˆ–é€‰ä¸­å†…å®¹
   è§¦å‘è¯: "æ€»ç»“", "æ¦‚æ‹¬", "æ‘˜è¦"
</tools>

<guidelines>
1. å½“ç”¨æˆ·çš„é—®é¢˜åŒ¹é…å·¥å…·è§¦å‘è¯æ—¶ï¼Œä½¿ç”¨å¯¹åº”å·¥å…·
2. æ™®é€šèŠå¤©ï¼ˆé—®å€™ã€é—²èŠã€ä¸€èˆ¬é—®é¢˜ï¼‰ç›´æ¥å›ç­”ï¼Œä¸éœ€è¦å·¥å…·
3. å·¥å…·è°ƒç”¨æˆåŠŸåï¼Œå‘Šè¯‰ç”¨æˆ·å…·ä½“åšäº†ä»€ä¹ˆ
4. æœç´¢æ— ç»“æœæ—¶ï¼Œå‘ŠçŸ¥ç”¨æˆ·å¹¶ç»™å‡ºå»ºè®®
5. ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„ç¬”è®°å†…å®¹
6. ä¿æŒå›å¤ç®€æ´è‡ªç„¶
</guidelines>

<output_format>
- ä½¿ç”¨ç®€æ´çš„ä¸­æ–‡å›å¤
- å¼•ç”¨ç¬”è®°æ—¶ç”¨ã€Œç¬”è®°æ ‡é¢˜ã€
- ä»£ç æˆ–æ ¼å¼åŒ–å†…å®¹ä½¿ç”¨ Markdown
- æ“ä½œæˆåŠŸç”¨ âœ…ï¼Œè­¦å‘Šç”¨ âš ï¸
</output_format>
</system>"""


class AgentSupervisor:
    """
    Hierarchical Agent Supervisor with enterprise-grade prompts.
    """
    
    def __init__(self):
        self.llm = get_llm()
        self._knowledge_worker = None
        self._note_worker = None
        self._format_worker = None
    
    def _get_knowledge_worker(self):
        if self._knowledge_worker is None:
            self._knowledge_worker = create_knowledge_worker(self.llm)
        return self._knowledge_worker
    
    def _get_note_worker(self):
        if self._note_worker is None:
            self._note_worker = create_note_worker(self.llm)
        return self._note_worker
    
    def _get_format_worker(self):
        if self._format_worker is None:
            self._format_worker = create_format_worker(self.llm)
        return self._format_worker

    def _prepare_history(self, history: Optional[List[Any]]) -> List[Any]:
        """Convert list of dicts or ChatMessage objects to LangChain message objects."""
        full_history = []
        if history:
            for h in history:
                # Handle both dict and Pydantic objects
                role = h.get("role", "user") if isinstance(h, dict) else getattr(h, "role", "user")
                content = h.get("content", "") if isinstance(h, dict) else getattr(h, "content", "")
                
                if role == "user":
                    full_history.append(HumanMessage(content=content))
                else:
                    full_history.append(AIMessage(content=content))
        return full_history
    
    async def _route_intent(self, message: str, history: List[Dict[str, str]] = None) -> str:
        """
        Semantic Intent Routing with Context Awareness.
        """
        history_context = ""
        if history:
            # Format last few messages for context
            ctx = (history or [])[-3:]
            history_context = "\n".join([f"{m.get('role', 'user')}: {m.get('content', '')[:100]}" for m in ctx])

        router_prompt = f"""<system>
ä½ æ˜¯ Origin Notes çš„æ™ºèƒ½è·¯ç”±ä¸­æ¢ã€‚åˆ†æç”¨æˆ·è¾“å…¥å’Œå¯¹è¯å†å²ï¼Œå°†æ„å›¾è·¯ç”±åˆ°æ­£ç¡®çš„ Workerã€‚

å¯é€‰ Workerï¼š
1. **knowledge** (çŸ¥è¯†æ£€ç´¢): æ£€ç´¢ã€åˆ—å‡ºã€æŸ¥çœ‹å­˜å‚¨çš„å¤šç¯‡å†…å®¹ã€‚
2. **note** (å†™æ“ä½œ): åˆ›å»ºã€ç¼–è¾‘æ ‡é¢˜/æ­£æ–‡ã€åˆ é™¤ç¬”è®°ã€‚
   - âš ï¸ **è§„åˆ™**: å‡¡æ˜¯é’ˆå¯¹å½“å‰æˆ–æŸç¯‡ç¬”è®°çš„â€œä¿®æ”¹â€ã€â€œæ”¹åâ€ã€â€œåˆ é™¤â€æŒ‡ä»¤ï¼Œå¿…é¡»ç”± **note** å¤„ç†ã€‚
3. **format** (æ’ç‰ˆ): ç¾åŒ–æ–‡æœ¬ã€‚
4. **summarize** (æ€»ç»“æç‚¼)ã€‚
5. **chat** (é€šç”¨é—®ç­”): é—²èŠã€ç§‘æ™®ã€‚

<context>
{history_context}
</context>

<instruction>
ä»…è¾“å‡º Worker åç§°ï¼ˆknowledge, note, format, summarize, chatï¼‰ã€‚
</instruction>
</system>

User: {message}
Router:"""
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=router_prompt)])
            intent = response.content.strip().lower()
            import re
            intent = re.sub(r'[^a-z]', '', intent)
            valid_intents = ["knowledge", "note", "format", "summarize", "chat"]
            return intent if intent in valid_intents else "chat"
        except:
            return "chat"

    async def _plan_knowledge_action(self, query: str) -> Dict[str, Any]:
        """Plan the knowledge retrieval strategy using LLM."""
        planner_prompt = f"""<system>
ä½ æ˜¯ Origin Notes çš„çŸ¥è¯†åº“è§„åˆ’å¸ˆã€‚åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œå†³å®šä½¿ç”¨å“ªç§æ£€ç´¢ç­–ç•¥ã€‚

å¯é€‰ç­–ç•¥ï¼š
1. **search**: ç”¨æˆ·åœ¨æŸ¥æ‰¾ç‰¹å®šçš„çŸ¥è¯†ç‚¹ã€è¯é¢˜æˆ–å…³é”®è¯ã€‚
   - è¾“å‡º: {{"action": "search", "query": "ä¼˜åŒ–çš„æœç´¢å…³é”®è¯"}}
   
2. **list_recent**: ç”¨æˆ·æƒ³çœ‹æœ€è¿‘å†™çš„ã€åˆšåˆ›å»ºçš„æˆ–æœ€æ–°çš„ç¬”è®°ã€‚
   - è¾“å‡º: {{"action": "list_recent", "limit": 8}}
   
3. **list_all**: ç”¨æˆ·æƒ³æµè§ˆæ‰€æœ‰ç¬”è®°ã€æˆ–è€…é—®æœ‰å“ªäº›ç¬”è®°ã€‚
   - è¾“å‡º: {{"action": "list_all", "limit": 10}}

<instruction>
è¯·ä»¥çº¯ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°ã€‚
</instruction>
</system>

User: {query}
Plan:"""
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=planner_prompt)])
            content = response.content.strip()
            
            # Clean markdown code blocks if present
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.replace("```", "").strip()
                
            import re
            # Try to extract JSON
            json_match = re.search(r'\{[^}]+\}', content)
            if json_match:
                plan = json.loads(json_match.group())
                return plan
            return json.loads(content)
        except Exception as e:
            print(f"âš ï¸ Planning failed, defaulting to search: {e}")
            return {"action": "search", "query": query}

    async def invoke(
        self,
        message: str,
        history: List[Dict[str, str]] = None,
        note_context: Optional[str] = None,
        selected_text: Optional[str] = None,
        active_note_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Invoke the agent with a user message."""
        
        # Build messages for context
        full_history = self._prepare_history(history)
        
        # Parse intent using LLM (The "True" Agentic Way)
        print(f"ğŸ§  Semantic Routing for: '{message}'")
        intent = await self._route_intent(message, history)
        print(f"ğŸ¯ Intent routed to: {intent}")
        
        # Build state
        state = {
            "messages": full_history + [HumanMessage(content=message)],
            "note_context": note_context,
            "active_note_id": active_note_id,
            "selected_text": selected_text,
            "worker_input": message,
            "response": "",
            "tool_calls": [],
        }
        
        try:
            if intent == "knowledge":
                print("ğŸ“š Routing to Knowledge Worker...")
                worker = self._get_knowledge_worker()
                result = await worker(state)
                return {
                    "response": result.get("response", "æœç´¢æ—¶å‡ºç°é—®é¢˜ã€‚"),
                    "tool_calls": ["search_notes"],
                }
            
            elif intent == "note":
                print("ğŸ“ Routing to Note Worker...")
                worker = self._get_note_worker()
                result = await worker(state)
                return {
                    "response": result.get("response", "ç¬”è®°æ“ä½œæ—¶å‡ºç°é—®é¢˜ã€‚"),
                    "tool_calls": ["create_note"],
                }
            
            elif intent == "summarize":
                print("ğŸ“‹ Routing to summarize...")
                if note_context:
                    # Summarize the current note
                    summary_prompt = f"è¯·ç®€æ´åœ°æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\n\n{note_context[:2000]}"
                    response = self.llm.invoke([
                        SystemMessage(content="ä½ æ˜¯æ€»ç»“ä¸“å®¶ã€‚ç”¨ç®€æ´çš„è¦ç‚¹æ€»ç»“å†…å®¹ã€‚"),
                        HumanMessage(content=summary_prompt)
                    ])
                    return {
                        "response": f"ğŸ“‹ **å†…å®¹æ‘˜è¦**\n\n{response.content}",
                        "tool_calls": ["summarize"],
                    }
                else:
                    return {
                        "response": "è¯·å…ˆæ‰“å¼€ä¸€ç¯‡ç¬”è®°ï¼Œæˆ‘æ‰èƒ½å¸®ä½ æ€»ç»“å†…å®¹ã€‚",
                        "tool_calls": [],
                    }
            
            elif intent == "format":
                print("âœ¨ Routing to Format Worker...")
                worker = self._get_format_worker()
                result = await worker(state)
                return {
                    "response": result.get("response", "æ ¼å¼åŒ–æ—¶å‡ºç°é—®é¢˜ã€‚"),
                    "tool_calls": ["format_text"],
                }
            
            else:
                # Direct chat - most common path
                print("ğŸ’¬ Direct chat response...")
                # Ensure we include the system prompt for persona consistency
                chat_messages = [SystemMessage(content=SUPERVISOR_PROMPT)] + full_history + [HumanMessage(content=message)]
                response = self.llm.invoke(chat_messages)
                return {
                    "response": response.content,
                    "tool_calls": [],
                }
        
        except Exception as e:
            print(f"âŒ Error in AgentSupervisor: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "response": "æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶å‡ºé”™äº†ã€‚è¯·ç¨åå†è¯•ã€‚",
                "tool_calls": ["error"],
            }
    
    async def stream(
        self,
        message: str,
        history: List[Dict[str, str]] = None,
        note_context: Optional[str] = None,
        selected_text: Optional[str] = None,
        active_note_id: Optional[str] = None,
    ) -> AsyncIterator[str]:
        """
        Stream agent responses with Semantic Routing.
        Matches the logic of invoke() but for streaming.
        """
        try:
            # 1. Route Intent
            intent = await self._route_intent(message, history)
            print(f"ğŸ§  Streaming Intent routed to: {intent}")

            # Common state for workers
            full_history = self._prepare_history(history)
            
            state = {
                "messages": full_history + [HumanMessage(content=message)],
                "note_context": note_context,
                "active_note_id": active_note_id,
                "selected_text": selected_text,
                "worker_input": message,
                "response": "",
                "tool_calls": [],
            }

            # Define status messages for each intent
            STATUS_MESSAGES = {
                "knowledge": "ğŸ“š æ­£åœ¨æœç´¢ç¬”è®°...",
                "note": "ğŸ“ æ­£åœ¨åˆ›å»ºç¬”è®°...",
                "format": "âœ¨ æ­£åœ¨ä¼˜åŒ–æ ¼å¼...",
                "summarize": "ğŸ“‹ æ­£åœ¨æ€»ç»“å†…å®¹...",
                "chat": "ğŸ’­ æ€è€ƒä¸­..."
            }
            
            # Send status message first (as JSON with type: status)
            status_msg = STATUS_MESSAGES.get(intent, "ğŸ’­ æ€è€ƒä¸­...")
            yield json.dumps({"type": "status", "text": status_msg})

            if intent == "knowledge":
                print("ğŸ“š Routing stream to Knowledge Worker...")
                # Get RAG service and plan
                from services.rag_service import RAGService
                rag_service = RAGService()
                await rag_service._ensure_loaded()
                
                # Plan the action using LLM
                plan = await self._plan_knowledge_action(message)
                print(f"ğŸ§  Knowledge Plan: {plan}")
                
                if plan['action'] in ['list_recent', 'list_all']:
                    # Static list - no LLM generation needed, yield directly
                    title_prefix = "ğŸ•’ **æœ€è¿‘çš„ç¬”è®°**" if plan['action'] == 'list_recent' else "ğŸ“š **ä½ çš„ç¬”è®°åˆ—è¡¨**"
                    notes = await rag_service.list_all_notes(limit=plan.get('limit', 8))
                    
                    if not notes:
                        yield "ç›®å‰è¿˜æ²¡æœ‰ä¿å­˜çš„ç¬”è®°ã€‚ä½ å¯ä»¥å¼€å§‹åˆ›å»ºæ–°ç¬”è®°ï¼"
                    else:
                        note_list = "\n".join([f"â€¢ **ã€Œ{n['title']}ã€**" for n in notes])
                        yield f"{title_prefix}ï¼ˆå…± {len(notes)} ç¯‡ï¼‰\n\n{note_list}\n\nğŸ’¡ ä½ å¯ä»¥ç›´æ¥é—®æˆ‘å…³äºè¿™äº›ç¬”è®°çš„å…·ä½“é—®é¢˜ã€‚"
                
                elif plan['action'] == 'search':
                    # Search + LLM synthesis - TRUE STREAMING
                    search_query = plan.get('query', message)
                    results = await rag_service.search(search_query, top_k=5)
                    print(f"ğŸ“Š Found {len(results)} results")
                    
                    if not results:
                        yield f"æŠ±æ­‰ï¼Œå…³äºã€Œ{search_query}ã€ï¼Œæˆ‘æ²¡æœ‰åœ¨ç¬”è®°ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"
                    else:
                        # Build context
                        context_parts = []
                        for r in results[:3]:
                            title = r.get('title', 'æ— æ ‡é¢˜')
                            content = r.get('content', '')[:1000]
                            context_parts.append(f"**ã€Œ{title}ã€**\n{content}")
                        context = "\n\n---\n\n".join(context_parts)
                        
                        # TRUE STREAMING with LLM
                        synthesis_prompt = f"ç”¨æˆ·é—®é¢˜ï¼š{message}\n\nå‚è€ƒç¬”è®°å†…å®¹ï¼š\n{context}\n\nè¯·æ ¹æ®ç¬”è®°å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                        async for chunk in self.llm.astream([
                            SystemMessage(content="ä½ æ˜¯çŸ¥è¯†æ£€ç´¢ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„ç¬”è®°å†…å®¹å›ç­”é—®é¢˜ï¼Œå¼•ç”¨æ—¶ä½¿ç”¨ã€Œç¬”è®°æ ‡é¢˜ã€æ ¼å¼ã€‚"),
                            HumanMessage(content=synthesis_prompt)
                        ]):
                            if chunk.content:
                                yield chunk.content
                else:
                    yield "æˆ‘ä¸ç¡®å®šè¯¥å¦‚ä½•æŸ¥æ‰¾è¯¥å†…å®¹ã€‚"
                
            elif intent == "note":
                print("ğŸ“ Routing stream to Note Worker...")
                worker = self._get_note_worker()
                result = await worker(state)
                yield result["response"]
                
            elif intent == "format":
                # Special WPS-style direct format brush
                print("âœ¨ Format brush triggered via stream...")
                formatted_md = await self.format_text(selected_text or note_context or message, note_context)
                
                # Convert to HTML for direct TipTap injection
                formatted_html = markdown.markdown(formatted_md, extensions=['fenced_code', 'tables', 'nl2br'])
                
                yield json.dumps({
                    "tool_call": "format_apply",
                    "formatted_html": formatted_html,
                    "formatted_md": formatted_md
                })
                
            elif intent == "summarize":
                # Summarize current note
                print("ğŸ“ Summarizing current note...")
                if note_context:
                    summary_prompt = f"è¯·æ€»ç»“ä»¥ä¸‹ç¬”è®°å†…å®¹çš„è¦ç‚¹ï¼š\n\n{note_context}"
                    async for chunk in self.llm.astream([
                        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¬”è®°æ€»ç»“ä¸“å®¶ã€‚è¯·ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€æ€»ç»“ç¬”è®°çš„æ ¸å¿ƒè¦ç‚¹ã€‚"),
                        HumanMessage(content=summary_prompt)
                    ]):
                        if chunk.content:
                            yield chunk.content
                else:
                    yield "è¯·å…ˆæ‰“å¼€ä¸€ç¯‡ç¬”è®°ï¼Œæˆ‘æ‰èƒ½ä¸ºä½ æ€»ç»“å†…å®¹ã€‚"
                
            else:
                # Direct chat with context awareness
                context_hint = ""
                if note_context:
                    context_hint = f"\n\n[å½“å‰ç”¨æˆ·æ­£åœ¨ç¼–è¾‘çš„ç¬”è®°å†…å®¹ï¼š\n{note_context[:2000]}...]"
                
                user_message = message + context_hint if context_hint else message
                chat_messages = [SystemMessage(content=SUPERVISOR_PROMPT)] + full_history + [HumanMessage(content=user_message)]
                
                async for chunk in self.llm.astream(chat_messages):
                    if chunk.content:
                        yield chunk.content
                        
        except Exception as e:
            print(f"âŒ Stream error: {e}")
            import traceback
            traceback.print_exc()
            yield f"æŠ±æ­‰ï¼Œå¤„ç†æµå¼è¯·æ±‚æ—¶é‡åˆ°é”™è¯¯: {str(e)}"
    
    async def format_text(
        self,
        text: str,
        context: Optional[str] = None,
    ) -> str:
        """Direct call to FormatWorker."""
        state = {
            "messages": [HumanMessage(content=f"è¯·æ ¼å¼åŒ–ï¼š\n\n{text}")],
            "selected_text": text,
            "note_context": context,
            "worker_input": text,
        }
        
        try:
            worker = self._get_format_worker()
            result = await worker(state)
            return result.get("response", text)
        except Exception as e:
            print(f"âŒ Error in format_text: {e}")
            return text
