"""
Agent Supervisor - Orchestrator of Thinking and Action.
Enterprise-grade implementation with Autonomous ReAct Loop.
"""
from typing import List, Dict, Any, Optional, AsyncIterator, Union
import asyncio
import json
import markdown

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage, BaseMessage
from langchain_core.utils.function_calling import convert_to_openai_tool

from core.config import settings
from core.llm import get_llm
from .tools import get_all_agent_tools


# Master System Prompt
SUPERVISOR_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ‹¥æœ‰è‡ªä¸»æ€§ã€æ€è€ƒèƒ½åŠ›çš„ä¼ä¸šçº§çŸ¥è¯†åŠ©æ‰‹ "Origin"ã€‚
ä½ çš„å·¥ä½œæ¨¡å¼æ˜¯åŸºäºŽ **ReAct (Reasoning and Acting)** æ¡†æž¶çš„ã€‚

### æ ¸å¿ƒå‡†åˆ™ï¼š
1. **å·¥å…·å†³ç­–è‡ªç†**ï¼šå½“ç”¨æˆ·æå‡ºé—®é¢˜ï¼Œä½ é¦–å…ˆåˆ†æžï¼šæˆ‘æ˜¯å¦éœ€è¦æŸ¥é˜…çŽ°æœ‰çš„ç¬”è®°ï¼Ÿè¿˜æ˜¯è¿™å±žäºŽâ€œé€šç”¨ç™¾ç§‘çŸ¥è¯†â€ï¼Ÿ
2. **ä¸¥é˜²â€œç§è´§â€é©±åŠ¨**ï¼š
   - å¯¹äºŽæ¶‰åŠç”¨æˆ·**ä¸ªäººèµ„äº§**ï¼ˆå¦‚â€œæˆ‘çš„è´¦å·â€ã€â€œæˆ‘æ˜¨å¤©çš„æ„Ÿæ‚Ÿâ€ï¼‰çš„é—®é¢˜ï¼Œ**å¿…é¡»**è°ƒç”¨å·¥å…·ï¼Œä¸¥ç¦ç¼–é€ ã€‚
   - å¯¹äºŽæ¶‰åŠ**å®¢è§‚é€šç”¨çŸ¥è¯†**ï¼ˆå¦‚â€œæ‹‰æ ¼æœ—æ—¥ä¸­å€¼å®šç†â€ã€â€œPython è¯­æ³•â€ï¼‰çš„é—®é¢˜ï¼Œå¦‚æžœå·¥å…·æœªæœåˆ°å†…å®¹ï¼Œä½ å¯ä»¥åŸºäºŽè‡ªèº«çŸ¥è¯†åº“å›žå¤ï¼Œä½†**å¿…é¡»å£°æ˜Ž**ï¼šâ€œåœ¨æ‚¨çš„ç¬”è®°ä¸­æœªæ‰¾åˆ°ç›¸å…³è®°å½•ï¼Œä»¥ä¸‹æ˜¯åŸºäºŽé€šç”¨çŸ¥è¯†çš„è§£ç­”â€ã€‚
4. **ä¸“ä¸šäº¤äº’**ï¼šæœ€ç»ˆå›žå¤å¿…é¡»é€»è¾‘æ¸…æ™°ã€‚å¦‚æžœæ˜¯å¯¹ç¬”è®°è¿›è¡Œäº†ä¼˜åŒ–æˆ–æ ¼å¼è°ƒæ•´ï¼Œåº”å½“æ˜Žç¡®æŒ‡å‡ºæ”¹è¿›äº†å“ªäº›åœ°æ–¹ã€‚
5. **æŒä¹…åŒ–ä¼˜å…ˆ**ï¼šå‡¡æ˜¯æ¶‰åŠâ€œä¿®æ”¹æ ¼å¼â€ã€â€œä¼˜åŒ–æŽ’ç‰ˆâ€ã€â€œæ•´ç†ç¬”è®°â€çš„è¦æ±‚ï¼Œå¿…é¡»é€šè¿‡ `update_note` å·¥å…·å°†ä¿®æ”¹ä¿å­˜åˆ°ç¼–è¾‘å™¨ä¸­ï¼Œç„¶åŽå†ç»™ç”¨æˆ·ä¸€æ®µè‡ªç„¶è¯­è¨€æ€»ç»“ã€‚

### å¤„ç†æµç¨‹ï¼š
- **Thought**: æ€è€ƒä¸‹ä¸€æ­¥è¯¥åšä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšã€‚
- **Action**: è°ƒç”¨æœ€åˆé€‚çš„å·¥å…·ï¼ˆsearch_knowledge, read_note_content, list_recent_notes ç­‰ï¼‰ã€‚
- **Observation**: è§‚å¯Ÿå·¥å…·åé¦ˆçš„æ•°æ®ã€‚
- **Final Answer**: åŸºäºŽäº‹å®žç»™å‡ºæœ€ç»ˆç»“è®ºã€‚

âš ï¸ **è­¦å‘Š**ï¼šå¦‚æžœå·¥å…·è¿”å›žâ€œæœªæ‰¾åˆ°å†…å®¹â€ï¼Œè¯·å¦‚å®žå‘ŠçŸ¥ï¼Œä¸¥ç¦è„‘è¡¥ã€‚
"""

class AgentSupervisor:
    """
    Autonomous Orchestrator using functional tool-calling and recursive reasoning.
    """
    
    def __init__(self):
        self.llm = get_llm()
        self.tools = get_all_agent_tools()
        # Bind tools to the model (OpenAI Protocol compatible)
        self.model_with_tools = self.llm.bind_tools(self.tools)
        # Internal map for execution
        self.tools_map = {tool.name: tool for tool in self.tools}
    
    def _prepare_history(self, history: Optional[List[Any]]) -> List[BaseMessage]:
        """Convert list of dicts or ChatMessage objects to LangChain message objects."""
        full_history = []
        if history:
            for h in history:
                role = h.get("role", "user") if isinstance(h, dict) else getattr(h, "role", "user")
                content = h.get("content", "") if isinstance(h, dict) else getattr(h, "content", "")
                
                if role == "user":
                    full_history.append(HumanMessage(content=content))
                else:
                    full_history.append(AIMessage(content=content))
        return full_history

    async def _execute_tool_call(self, tool_call: Dict[str, Any]) -> str:
        """Execute a single tool call and return the result as string."""
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        
        if tool_name not in self.tools_map:
            return f"Error: Tool {tool_name} not found."
            
        try:
            tool = self.tools_map[tool_name]
            # Execute async tool
            result = await tool.ainvoke(tool_args)
            
            # Special case for JSON string results (like update_note)
            if isinstance(result, str) and result.startswith("{"):
                return result
            return str(result)
        except Exception as e:
            return f"Error executing {tool_name}: {str(e)}"

    async def invoke_stream(
        self,
        message: str,
        history: Optional[List[Dict[str, Any]]] = None,
        note_context: Optional[str] = None,
        active_note_id: Optional[str] = None,
        selected_text: Optional[str] = None,
    ) -> AsyncIterator[str]:
        """
        Multi-turn Autonomous Execution Loop.
        Implements the Reasoning -> Acting -> Observation loop.
        """
        try:
            # 1. Setup Initial State
            full_history = self._prepare_history(history)
            
            # Inject dynamic situational awareness
            current_situation = f"\n\n[Current Context]\nActive Note ID: {active_note_id or 'None'}\n"
            if note_context:
                current_situation += f"Quick Preview of Active Note (First 500 chars): {note_context[:500]}...\n"
                
            messages = [
                SystemMessage(content=SUPERVISOR_PROMPT + current_situation),
            ] + full_history + [
                HumanMessage(content=message)
            ]

            max_turns = 3 # Reduce turns to avoid excessive searching for common knowledge
            turn = 0
            
            while turn < max_turns:
                turn += 1
                
                # Check if this is the last chance to answer
                is_last_turn = (turn == max_turns)
                
                # UI Feedback
                if turn == 1:
                    yield json.dumps({"type": "status", "text": "ðŸ§  æ€è€ƒä¸­..."})
                
                # Ask the model (Thinking step)
                # If it's the last turn, we append a final instruction to stop tool use
                current_messages = messages
                if is_last_turn:
                    current_messages = messages + [HumanMessage(content="[SYSTEM]: æœç´¢æ¬¡æ•°å·²è¾¾ä¸Šé™ã€‚è¯·ä¸è¦å†è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œç›´æŽ¥åŸºäºŽçŽ°æœ‰ä¿¡æ¯æˆ–ä½ çš„é€šç”¨èƒŒæ™¯çŸ¥è¯†ç»™å‡ºæœ€ç»ˆå›žç­”ã€‚")]

                ai_msg = await self.model_with_tools.ainvoke(current_messages)
                
                # Case A: Model wants to call tools (and we haven't hit the limit yet)
                if ai_msg.tool_calls and not is_last_turn:
                    messages.append(ai_msg)
                    
                    for tool_call in ai_msg.tool_calls:
                        tool_name = tool_call["name"]
                        
                        # UI Feedback
                        STATUS_LABELS = {
                            "search_knowledge": "ðŸ“š æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“...",
                            "read_note_content": "ðŸ“– æ­£åœ¨è¯»å–ç¬”è®°å…¨æ–‡...",
                            "list_recent_notes": "ðŸ“ æ­£åœ¨å¯»æ‰¾ç¬”è®°...",
                            "update_note": "âš™ï¸ æ­£åœ¨æ‰§è¡Œç¬”è®°æ›´æ–°...",
                            "create_note": "ðŸ†• æ­£åœ¨åˆ›å»ºæ–°ç¬”è®°...",
                            "delete_note": "ðŸ—‘ï¸ æ­£åœ¨æ¸…ç†ç¬”è®°..."
                        }
                        yield json.dumps({"type": "status", "text": STATUS_LABELS.get(tool_name, f"ðŸ› ï¸ è°ƒç”¨ {tool_name}...")})
                        
                        # Execute
                        observation = await self._execute_tool_call(tool_call)
                        
                        # High-End UX: Trigger UI refresh for ALL data mutations
                        try:
                            if tool_name == "update_note" and "Successfully updated" in observation:
                                from services.note_service import NoteService
                                ns = NoteService()
                                note_data = await ns.get_note(tool_call["args"].get("note_id"))
                                if note_data:
                                    html_content = markdown.markdown(note_data.get("content", ""), extensions=['fenced_code', 'tables', 'nl2br'])
                                    yield json.dumps({"tool_call": "format_apply", "formatted_html": html_content})
                            
                            elif tool_name == "create_note" and "Successfully created" in observation:
                                # Extract ID using regex: ID: ([\w-]+)
                                match = re.search(r"ID:\s*([\w-]+)", observation)
                                note_id = match.group(1) if match else None
                                yield json.dumps({"tool_call": "note_created", "note_id": note_id, "message": "New note created and synced."})
                            
                            elif tool_name == "delete_note" and "Successfully deleted" in observation:
                                note_id = tool_call["args"].get("note_id")
                                yield json.dumps({"tool_call": "note_deleted", "note_id": note_id, "message": "Note deleted from library."})
                        except Exception as sync_err:
                            print(f"[WARN] UI Sync Warning: {sync_err}")

                        messages.append(ToolMessage(content=observation, tool_call_id=tool_call["id"]))
                    
                    # Continue for next turn
                    continue
                
                # Case B: Model gives a final answer OR we forced it on the last turn
                else:
                    # Clear status for final output
                    yield json.dumps({"type": "status", "text": ""})
                    
                    # 1. First, yield whatever content we already got from ainvoke
                    if ai_msg.content:
                        yield ai_msg.content
                    
                    # 2. If it was a forced turn and content was empty, or we want a synthesis flow, 
                    # we could stream, but usually ai_msg.content has the answer now.
                    # Only stream if ai_msg.content is surprisingly short/missing
                    if not ai_msg.content.strip():
                        async for chunk in self.llm.astream(messages):
                            if chunk.content:
                                yield chunk.content
                    
                    return # Exit after final answer
            
            # Fallback if loop finishes without yield (should not happen with else block logic)
            yield "æŠ±æ­‰ï¼Œä»»åŠ¡å¤„ç†è½®æ¬¡è¶…é™ï¼Œæœªèƒ½ç”Ÿæˆæœ‰æ•ˆå›žç­”ã€‚è¯·å°è¯•æ¢ä¸ªé—®æ³•ã€‚"
                    
        except Exception as e:
            print(f"[ERR] Orchestration Error: {e}")
            import traceback
            traceback.print_exc()
            yield f"æŠ±æ­‰ï¼Œç³»ç»Ÿé€»è¾‘å±‚å‡ºçŽ°é”™è¯¯ï¼š{str(e)}"

    async def invoke(self, *args, **kwargs) -> Dict[str, Any]:
        """Legacy compatibility for non-streaming calls."""
        # Simple implementation: collect stream and return
        full_text = ""
        async for chunk in self.invoke_stream(*args, **kwargs):
            if not chunk.startswith("{"):
                full_text += chunk
        return {"response": full_text, "tool_calls": []}
