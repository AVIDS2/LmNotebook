"""
Knowledge Worker - Enterprise-grade Agentic RAG implementation.
Optimized for Gemini and intelligent retrieval.
"""
from typing import Dict, Any, Callable, List
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

from services.rag_service import RAGService


# Enterprise Knowledge Worker Prompt
KNOWLEDGE_SYSTEM_PROMPT = """<system>
ä½ æ˜¯çŸ¥è¯†æ£€ç´¢ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åœ¨ç”¨æˆ·çš„ç¬”è®°åº“ä¸­æ‰¾åˆ°æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

<workflow>
1. åˆ†æç”¨æˆ·é—®é¢˜çš„çœŸæ­£æ„å›¾
2. å°†é—®é¢˜æ”¹å†™ä¸ºæ›´å¥½çš„æœç´¢æŸ¥è¯¢ï¼ˆè¯­ä¹‰å…³é”®è¯ï¼‰
3. æ‰§è¡Œæœç´¢
4. ç»¼åˆå›ç­”ï¼Œå¼•ç”¨å…·ä½“æ¥æº
</workflow>

<rules>
- æ°¸è¿œä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„å†…å®¹
- å¦‚æœæœç´¢æ— ç»“æœï¼Œæ˜ç¡®å‘Šè¯‰ç”¨æˆ·
- ç»™å‡ºçš„ä¿¡æ¯å¿…é¡»æ¥è‡ªå®é™…çš„ç¬”è®°
- å¼•ç”¨ç¬”è®°æ—¶ä½¿ç”¨ã€Œç¬”è®°æ ‡é¢˜ã€æ ¼å¼
- å¦‚æœç”¨æˆ·åªæ˜¯æƒ³çœ‹æ‰€æœ‰çš„ç¬”è®°æˆ–æœ€è¿‘çš„ç¬”è®°ï¼Œè¯·å‹å–„åœ°åˆ—å‡ºæ¥
</rules>
</system>"""


def create_knowledge_worker(llm) -> Callable:
    """Create a Knowledge Worker for RAG-based note search."""
    rag_service = RAGService()
    
    async def knowledge_worker(state: Dict[str, Any]) -> Dict[str, Any]:
        """Execute knowledge search using LLM planning."""
        messages = state.get("messages", [])
        worker_input = state.get("worker_input", "")
        
        # Get the user's query
        query = worker_input
        if not query and messages:
            for msg in reversed(messages):
                if isinstance(msg, HumanMessage):
                    query = msg.content
                    break
        
        if not query:
            return {
                "response": "è¯·å‘Šè¯‰æˆ‘ä½ æƒ³æœç´¢ä»€ä¹ˆå†…å®¹ã€‚",
                "tool_calls": ["search_notes"],
            }
        
        print(f"ğŸ” Knowledge Agent processing: {query}")
        
        # Enterprise Pattern: LLM decides the strategy (Search vs List vs Recent)
        plan = await _plan_knowledge_action(llm, query)
        print(f"ğŸ§  Knowledge Plan: {plan}")
        
        if plan['action'] == 'list_recent':
            return await _list_notes(rag_service, title_prefix="ğŸ•’ **æœ€è¿‘çš„ç¬”è®°**", limit=plan.get('limit', 8))
            
        elif plan['action'] == 'list_all':
            return await _list_notes(rag_service, title_prefix="ğŸ“š **ä½ çš„ç¬”è®°åˆ—è¡¨**", limit=plan.get('limit', 10))
            
        elif plan['action'] == 'search':
            # Execute semantic search with the optimized query from the plan
            search_query = plan.get('query', query)
            results = await rag_service.search(search_query, top_k=5)
            print(f"ğŸ“Š Found {len(results)} results")
            
            if not results:
                response = f"æŠ±æ­‰ï¼Œå…³äºã€Œ{search_query}ã€ï¼Œæˆ‘æ²¡æœ‰åœ¨ç¬”è®°ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"
            else:
                response = await _synthesize_response(llm, query, results)
                
            return {
                "response": response,
                "tool_calls": ["search_notes"],
            }
            
        else:
            # Fallback
            return {
                "response": "æˆ‘ä¸ç¡®å®šè¯¥å¦‚ä½•æŸ¥æ‰¾è¯¥å†…å®¹ã€‚",
                "tool_calls": [],
            }

    return knowledge_worker


async def _plan_knowledge_action(llm, query: str) -> Dict[str, Any]:
    """Use LLM to plan the knowledge retrieval strategy."""
    # NOTE: Braces in JSON examples MUST be escaped as {{ }} for .format() to work
    PLANNER_PROMPT = """<system>
ä½ æ˜¯ Origin Notes çš„çŸ¥è¯†åº“è§„åˆ’å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œå¹¶å†³å®šä½¿ç”¨å“ªç§æ£€ç´¢ç­–ç•¥ã€‚

å¯é€‰ç­–ç•¥ (Action)ï¼š
1. **search**: ç”¨æˆ·åœ¨æŸ¥æ‰¾ç‰¹å®šçš„çŸ¥è¯†ç‚¹ã€è¯é¢˜æˆ–å…³é”®è¯ã€‚
   - è¾“å‡º: {{"action": "search", "query": "ä¼˜åŒ–çš„æœç´¢å…³é”®è¯"}}
   
2. **list_recent**: ç”¨æˆ·æƒ³çœ‹æœ€è¿‘å†™çš„ã€åˆšåˆ›å»ºçš„æˆ–æœ€æ–°çš„ç¬”è®°ã€‚
   - è¾“å‡º: {{"action": "list_recent", "limit": 8}}
   
3. **list_all**: ç”¨æˆ·æƒ³æµè§ˆæ‰€æœ‰ç¬”è®°ã€æˆ–è€…é—®æœ‰å“ªäº›ç¬”è®°ã€‚
   - è¾“å‡º: {{"action": "list_all", "limit": 10}}

<instruction>
è¯·ä»¥çº¯ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°ã€‚
</instruction>
</system>

User: {query}
Plan:"""

    try:
        response = await llm.ainvoke([HumanMessage(content=PLANNER_PROMPT.format(query=query))])
        content = response.content.strip()
        
        # Clean markdown code blocks if present
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.replace("```", "").strip()
            
        import json
        plan = json.loads(content)
        return plan
    except Exception as e:
        print(f"âš ï¸ Planning failed, defaulting to search: {e}")
        return {"action": "search", "query": query}


async def _list_notes(rag_service: RAGService, title_prefix="ğŸ“š **ä½ çš„ç¬”è®°**", limit=8) -> Dict[str, Any]:
    """List available notes."""
    notes = await rag_service.list_all_notes(limit=limit)
    
    print(f"ğŸ“ _list_notes called, got {len(notes) if notes else 0} notes")
    if notes:
        for n in notes[:3]:
            print(f"   - Title: {n.get('title', 'MISSING')}, ID: {n.get('id', 'MISSING')}")
    
    if not notes:
        return {
            "response": "ç›®å‰è¿˜æ²¡æœ‰ä¿å­˜çš„ç¬”è®°ã€‚ä½ å¯ä»¥å¼€å§‹åˆ›å»ºæ–°ç¬”è®°ï¼",
            "tool_calls": ["search_notes"],
        }
    
    note_list = "\n".join([f"â€¢ **ã€Œ{n['title']}ã€**" for n in notes])
    response = f"{title_prefix}ï¼ˆå…± {len(notes)} ç¯‡ï¼‰\n\n{note_list}\n\nğŸ’¡ ä½ å¯ä»¥ç›´æ¥é—®æˆ‘å…³äºè¿™äº›ç¬”è®°çš„å…·ä½“é—®é¢˜ã€‚"
    
    print(f"ğŸ“ Response preview: {response[:200]}...")
    
    return {
        "response": response,
        "tool_calls": ["search_notes"],
    }


async def _synthesize_response(llm, query: str, results: list) -> str:
    """Synthesize a helpful response from search results."""
    # Build context from results
    context_parts = []
    for i, r in enumerate(results[:3], 1):
        title = r.get('title', 'æ— æ ‡é¢˜')
        content = r.get('content', '')[:1000] # Gemini has large context
        context_parts.append(f"**ã€Œ{title}ã€**\n{content}")
    
    context = "\n\n---\n\n".join(context_parts)
    
    try:
        response = llm.invoke([
            SystemMessage(content=KNOWLEDGE_SYSTEM_PROMPT),
            HumanMessage(content=f"ç”¨æˆ·è¦æ±‚ï¼š{query}\n\nå‚è€ƒç¬”è®°å†…å®¹ï¼š\n{context}\n\nè¯·æ ¹æ®ç¬”è®°å›ç­”ã€‚")
        ])
        return response.content
    except Exception as e:
        # Fallback: just show the results
        return f"ğŸ“š **å‚è€ƒç¬”è®°**\n\n{context}"
