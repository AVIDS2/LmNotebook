"""
Knowledge Worker - Enterprise-grade Agentic RAG implementation.
Optimized for Gemini and intelligent retrieval.
"""
from typing import Dict, Any, Callable, List
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

from core.config import settings
from services.rag_service import RAGService


# Enterprise Knowledge Worker Prompt
KNOWLEDGE_SYSTEM_PROMPT = """<system>
ä½ æ˜¯çŸ¥è¯†æ£€ç´¢ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åœ¨ç”¨æˆ·çš„ç¬”è®°åº“ä¸­æ‰¾åˆ°æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

<workflow>
1. åˆ†æç”¨æˆ·é—®é¢˜çš„çœŸæ­£æ„å›¾
2. å°†é—®é¢˜æ”¹å†™ä¸ºæ›´å¥½çš„æœç´¢æŸ¥è¯¢ï¼ˆè¯­ä¹‰å…³é”®è¯ï¼‰
3. æ‰§è¡Œæœç´¢
4. ç»¼åˆå›ç­”ï¼Œå¼•ç”¨å…·ä½“æ¥æº
</workflow>

<rules>
- **ä¸¥æ ¼éµå¾ªä¸Šä¸‹æ–‡**ï¼šå›ç­”å¿…é¡»åŸºäºä¸‹æ–¹æä¾›çš„ã€Œå‚è€ƒç¬”è®°å†…å®¹ã€ã€‚
- **ç¦æ­¢ç¼–é€ **ï¼šå¦‚æœå‚è€ƒç¬”è®°ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥å›ç­”â€œæŠ±æ­‰ï¼Œç¬”è®°ä¸­æ²¡æœ‰æåˆ°ç›¸å…³å†…å®¹â€ï¼Œä¸¥ç¦ä½¿ç”¨ä½ çš„é€šç”¨çŸ¥è¯†å»ç¼–é€ ç­”æ¡ˆã€‚
- **å¼•ç”¨ä¼˜å…ˆ**ï¼šå¼•ç”¨ç¬”è®°æ—¶ä½¿ç”¨ã€Œç¬”è®°æ ‡é¢˜ã€æ ¼å¼ã€‚
- **è¯­æ°”è‡ªç„¶**ï¼šåƒä¸ªåŠ©æ‰‹ä¸€æ ·äº¤æµï¼Œä½†äº‹å®å¿…é¡»ç»å¯¹å‡†ç¡®ã€‚
</rules>
</system>"""


def create_knowledge_worker(llm) -> Callable:
    """Create a Knowledge Worker for RAG-based note search."""
    rag_service = RAGService()
    
    async def knowledge_worker(state: Dict[str, Any]) -> Dict[str, Any]:
        """Execute knowledge search using LLM planning."""
        messages = state.get("messages", [])
        worker_input = state.get("worker_input", "")
        
        # Get the user's query
        query = worker_input
        if not query and messages:
            for msg in reversed(messages):
                if isinstance(msg, HumanMessage):
                    query = msg.content
                    break
        
        if not query:
            return {
                "response": "è¯·å‘Šè¯‰æˆ‘ä½ æƒ³æœç´¢ä»€ä¹ˆå†…å®¹ã€‚",
                "tool_calls": ["search_notes"],
            }
        
        print(f"[SEARCH] Knowledge Agent processing: {query}")
        
        # Enterprise Pattern: LLM decides the strategy (Search vs List vs Recent)
        plan = await _plan_knowledge_action(llm, query)
        print(f"[PLAN] Knowledge Plan: {plan}")
        
        if plan['action'] == 'list_recent':
            return await _list_notes(rag_service, llm, query, title_prefix="ğŸ•’ **æœ€è¿‘çš„ç¬”è®°**", limit=plan.get('limit', 8), require_summary=plan.get('require_summary', False))
            
        elif plan['action'] == 'list_all':
            return await _list_notes(rag_service, llm, query, title_prefix="ğŸ“š **ä½ çš„ç¬”è®°åˆ—è¡¨**", limit=plan.get('limit', 10), require_summary=plan.get('require_summary', False))
            
        elif plan['action'] == 'search':
            # Execute semantic search
            search_query = plan.get('query', query)
            results = await rag_service.search(search_query, top_k=settings.TOP_K_RESULTS)
            
            # Fallback Pattern: "Exact Title Match"
            # If user asks about a specific note (e.g. Analysis of "Rust Note"), fetch full content
            # because vector chunks might be too fragmented.
            import re
            title_match = re.search(r'[ã€Œã€Š](.*?)[ã€ã€‹]', query)
            if title_match:
                specific_title = title_match.group(1)
                print(f"[READ] Detected specific note title: {specific_title}, fetching full content...")
                from services.note_service import NoteService
                note_svc = NoteService()
                # We need a method to find by title, for now let's iterate or rely on search result ID if available
                # Optimization: check if any search result has high similarity to title
                target_note = None
                
                # First check if it's already in results
                for r in results:
                    if specific_title in r['title']:
                        target_note = r
                        break
                
                # If not in vectors, try DB search (fuzzy)
                if not target_note:
                    all_notes = await note_svc.get_all_notes()
                    for n in all_notes:
                        if specific_title in n['title']:
                            # Fetch full content
                            full = await note_svc.get_note(n['id'])
                            target_note = {"title": n['title'], "content": full['plainText']}
                            break
                
                if target_note:
                    # Prepend the FULL content of the specific note to results
                    # This gives LLM the complete context to answer "specific cases"
                    results.insert(0, target_note)

            if not results:
                response = f"æŠ±æ­‰ï¼Œå…³äºã€Œ{search_query}ã€ï¼Œæˆ‘ç›®å‰æ²¡åœ¨ç¬”è®°ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"
            else:
                response = await _synthesize_response(llm, query, results)
                
            return {
                "response": response,
                "tool_calls": ["search_notes"],
            }
            
        else:
            # Fallback
            return {
                "response": "æˆ‘ä¸ç¡®å®šè¯¥å¦‚ä½•æŸ¥æ‰¾è¯¥å†…å®¹ã€‚",
                "tool_calls": [],
            }

    return knowledge_worker


async def _plan_knowledge_action(llm, query: str) -> Dict[str, Any]:
    """Use LLM to plan the knowledge retrieval strategy."""
    # NOTE: Braces in JSON examples MUST be escaped as {{ }} for .format() to work
    PLANNER_PROMPT = """<system>
ä½ æ˜¯ Origin Notes çš„çŸ¥è¯†åº“è§„åˆ’å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œå¹¶å†³å®šä½¿ç”¨å“ªç§æ£€ç´¢ç­–ç•¥ã€‚

å¯é€‰ç­–ç•¥ (Action)ï¼š
1. **search**: ç”¨æˆ·åœ¨æŸ¥æ‰¾ç‰¹å®šçš„çŸ¥è¯†ç‚¹ã€è¯é¢˜æˆ–å…³é”®è¯ã€‚
   - è¾“å‡º: {{"action": "search", "query": "ä¼˜åŒ–çš„æœç´¢å…³é”®è¯"}}
   
2. **list_recent**: ç”¨æˆ·æƒ³çœ‹æœ€è¿‘å†™çš„ã€åˆšåˆ›å»ºçš„æˆ–æœ€æ–°çš„ç¬”è®°ã€‚
   - è¾“å‡º: {{"action": "list_recent", "limit": 8}}
   
3. **list_all**: ç”¨æˆ·æƒ³æµè§ˆæ‰€æœ‰ç¬”è®°ã€æˆ–è€…é—®æœ‰å“ªäº›ç¬”è®°ã€‚
   - è¾“å‡º: {{"action": "list_all", "limit": 10}}

4. **summary**: å¦‚æœç”¨æˆ·ä¸ä»…æƒ³çœ‹åˆ—è¡¨ï¼Œè¿˜æƒ³çœ‹**æ¦‚æ‹¬**ã€**å¤§çº²**ã€**æ€»ç»“**æˆ–**ä»‹ç»**ã€‚
   - åœ¨ä¸Šè¿° action ä¸­å¢åŠ å­—æ®µ: "require_summary": true
   - ä¾‹å¦‚: {{"action": "list_all", "limit": 10, "require_summary": true}}

<instruction>
è¯·ä»¥çº¯ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°ã€‚
</instruction>
</system>

User: {query}
Plan:"""

    try:
        # ä½¿ç”¨ .replace è€Œä¸æ˜¯ .formatï¼Œä»¥é¿å¼€ prompt é‡Œçš„ JSON å¤§æ‹¬å·å†²çª
        formatted_prompt = PLANNER_PROMPT.replace("{query}", query)
        response = await llm.ainvoke([HumanMessage(content=formatted_prompt)])
        content = response.content.strip()
        
        # Clean markdown code blocks if present
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.replace("```", "").strip()
            
        import json
        plan = json.loads(content)
        return plan
    except Exception as e:
        print(f"[WARN] Planning failed, defaulting to search: {e}")
        return {"action": "search", "query": query}


async def _list_notes(rag_service: RAGService, llm, query: str, title_prefix="ğŸ“š **ä½ çš„ç¬”è®°**", limit=8, require_summary=False) -> Dict[str, Any]:
    """List notes, optionally summarizing them if requested by Planner."""
    notes = await rag_service.list_all_notes(limit=limit)
    
    if not notes:
        return {
            "response": "ç›®å‰è¿˜æ²¡æœ‰ä¿å­˜çš„ç¬”è®°ã€‚ä½ å¯ä»¥å¼€å§‹åˆ›å»ºæ–°ç¬”è®°ï¼",
            "tool_calls": ["search_notes"],
        }

    # SIMPLE LIST MODE
    if not require_summary:
        note_list = "\n".join([f"â€¢ **ã€Œ{n['title']}ã€**" for n in notes])
        response = f"{title_prefix}ï¼ˆå…± {len(notes)} ç¯‡ï¼‰\n\n{note_list}\n\nğŸ’¡ ä½ å¯ä»¥ç›´æ¥é—®æˆ‘å…³äºè¿™äº›ç¬”è®°çš„å…·ä½“é—®é¢˜ã€‚"
        return {"response": response, "tool_calls": ["search_notes"]}

    # DEEP SUMMARY MODE
    # If user wants summary/outline, we must fetch CONTENT.
    # To avoid context explosion, we limit to top 5 notes for summary or just read first 500 chars
    print("[THINK] Generating deep summary for notes listing...")
    
    # Fetch content (RAGService list_all_notes usually returns content="" for perf, so we might need re-fetch or use what we have)
    # The current list_all_notes implementation (based on previous edits) might return empty content.
    # Let's assume we need to fetch. Ideally rag_service should support this, but for now let's try direct DB fetch if content is missing
    # Or just rely on what we have if rag_service was updated.
    # Safety: let's re-fetch via ID to be sure
    from services.note_service import NoteService
    note_svc = NoteService()
    
    context_parts = []
    for n in notes[:6]: # Limit to 6 to save tokens
        full_note = await note_svc.get_note(n['id'])
        if full_note:
            text = full_note.get('plainText', '')[:2500] # Expanded to 2500 chars for deeper summary
            context_parts.append(f"æ ‡é¢˜ï¼š{n['title']}\nå†…å®¹æ‘˜è¦ï¼š{text}...")
            
    context = "\n\n".join(context_parts)
    
    summary_prompt = f"""<system>
ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“æ•´ç†ä¸“å®¶ã€‚ç”¨æˆ·å¸Œæœ›æŸ¥çœ‹ç¬”è®°åˆ—è¡¨çš„â€œæ¦‚æ‹¬â€æˆ–â€œå¤§çº²â€ã€‚
è¯·åŸºäºä»¥ä¸‹ç¬”è®°å†…å®¹ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„çŸ¥è¯†åº“æ¦‚è§ˆã€‚
å¯¹æ¯ç¯‡ç¬”è®°ç”¨ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒã€‚
</system>

ç¬”è®°åˆ—è¡¨æ•°æ®ï¼š
{context}

ç”¨æˆ·æŒ‡ä»¤ï¼š{query}

è¾“å‡ºæ ¼å¼ï¼š
### ğŸ“š çŸ¥è¯†åº“æ¦‚è§ˆ
- **[æ ‡é¢˜]**: æ ¸å¿ƒå†…å®¹ä¸€å¥è¯æ€»ç»“...
...
"""
    response_msg = await llm.ainvoke([HumanMessage(content=summary_prompt)])
    
    return {
        "response": response_msg.content,
        "tool_calls": ["search_notes"],
    }


async def _synthesize_response(llm, query: str, results: list) -> str:
    """Synthesize a helpful response from search results."""
    # Build context from results
    context_parts = []
    for i, r in enumerate(results[:3], 1):
        title = r.get('title', 'æ— æ ‡é¢˜')
        content = r.get('content', '')[:1000] # Gemini has large context
        context_parts.append(f"**ã€Œ{title}ã€**\n{content}")
    
    context = "\n\n---\n\n".join(context_parts)
    
    try:
        response = llm.invoke([
            SystemMessage(content=KNOWLEDGE_SYSTEM_PROMPT),
            HumanMessage(content=f"ç”¨æˆ·è¦æ±‚ï¼š{query}\n\nå‚è€ƒç¬”è®°å†…å®¹ï¼š\n{context}\n\nè¯·æ ¹æ®ç¬”è®°å›ç­”ã€‚")
        ])
        return response.content
    except Exception as e:
        # Fallback: just show the results
        return f"ğŸ“š **å‚è€ƒç¬”è®°**\n\n{context}"
